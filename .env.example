# OpenRouter API Key
# Get your API key from: https://openrouter.ai/keys
OPENROUTER_API_KEY=your_openrouter_api_key_here

# ===== YOLO DETECTION (OPTIONAL) =====
# Roboflow API Key for Warhammer miniature detection
# Get your API key from: https://roboflow.com/
# Model: Warhammer 40K Miniature Wargame (97 images)
# Benefits: Better bbox accuracy, handles dynamic poses, faster, no token costs
# If not set, falls back to Claude Haiku for detection
ROBOFLOW_API_KEY=
ROBOFLOW_MODEL_ENDPOINT=https://detect.roboflow.com/warhammer-40.000-miniature/1

# ===== SINGLE-STAGE MODE CONFIGURATION =====
# Used when USE_TWO_TIER=false
# Available models (with vision support):
# - anthropic/claude-3.5-sonnet (latest, Oct 2024 - recommended)
# - anthropic/claude-3.5-sonnet-20240620 (June 2024)
# - anthropic/claude-sonnet-4.5 (newest, if available)
# - anthropic/claude-3-haiku-20240307 (fast & cheap)
# - openai/gpt-4-vision-preview (alternative)
OPENROUTER_MODEL=anthropic/claude-3.5-sonnet

# ===== TWO-TIER CASCADE CONFIGURATION =====
# Used when USE_TWO_TIER=true

# PASS 1: Fast detection model (cheap, fast, ~$0.001/image)
# Detects miniatures and provides rough classifications
# Recommended: google/gemini-2.5-flash-lite
# Alternatives:
# - google/gemini-flash-1.5 (older version)
# - meta-llama/llama-3.2-90b-vision-instruct (more expensive but good)
PASS1_MODEL=google/gemini-2.5-flash-lite

# PASS 2: Precise classification model (accurate, ~$0.004/image)
# Verifies counts and provides exact unit names and factions
# Recommended: anthropic/claude-3.5-sonnet
# Alternatives:
# - anthropic/claude-3-opus (most accurate, slower, expensive)
# - anthropic/claude-3-haiku (faster, cheaper, less accurate)
PASS2_MODEL=anthropic/claude-3.5-sonnet

# Smart Escalation: Skip PASS 2 if PASS 1 confidence is high enough
# This saves costs by only running the expensive model when needed
# - 0.85 (85%): Skip ~40-60% of PASS 2 calls (recommended)
# - 1.0: Always run PASS 2 (no skipping, maximum accuracy)
# - 0.0: Always skip PASS 2 (PASS 1 only - not recommended)
CONFIDENCE_THRESHOLD=0.85

# ===== SERVER CONFIGURATION =====
PORT=3001

# ===== ANALYSIS MODE =====
# Bbox-based analyzer (RECOMMENDED - fixes overcounting issues)
# Uses count-index lock pattern to prevent LLM hallucination of extra miniatures
# Three-pass pipeline:
#   PASS 1: Bbox detection (establishes COUNT AUTHORITY)
#   PASS 2: Per-crop classification (cannot add/remove detections)
#   PASS 3: Triangulation (second opinion for ambiguous cases)
#
# Benefits:
#   - 100% accurate counting (no hallucinations)
#   - Verifiable count integrity at each phase
#   - Smart triangulation only when needed
#
# DEFAULT: true (bbox mode is most accurate)
USE_BBOX=true

# Legacy modes (for comparison/backward compatibility)
# Two-tier cascade: Set to 'true' to use Gemini Flash → Claude classification
USE_TWO_TIER=false
# Single-stage: Both false = original single-stage Claude

# ===== BBOX-BASED ANALYZER CONFIGURATION =====
# Used when USE_BBOX=true

# PASS 1: Bbox Detection Model (~$0.02/image with Claude Haiku)
# Detects bounding boxes and establishes count authority
BBOX_MODEL=anthropic/claude-3-5-haiku
# Note: Originally planned to use qwen/qwen2.5-vl-3b-instruct (free tier)
# but Qwen VL models not currently available on OpenRouter

# PASS 2: Classification Model (~$0.10/image for 2 crops)
# Classifies each crop individually (NO COUNTING ALLOWED)
CLASSIFIER_MODEL=anthropic/claude-3.5-sonnet

# PASS 3: Triangulation Model (~$0.08/image per ambiguous crop)
# Provides second opinion for low-confidence classifications
TRIANGULATION_MODEL=meta-llama/llama-3.2-90b-vision-instruct

# Detection thresholds
BBOX_IOU_THRESHOLD=0.5                # NMS overlap threshold (0.0-1.0)

# Classification thresholds
TRIANGULATION_THRESHOLD=0.7           # Min confidence to skip triangulation (0.0-1.0)
TRIANGULATION_MARGIN=0.15             # Min difference to arbitrate (0.0-1.0)

# Clump separation (for dense miniature groups)
ENABLE_CLUMP_SEPARATION=true          # Enable/disable clump detection
CLUMP_AREA_RATIO=3.0                  # Area threshold (bbox > N * median)
CLUMP_ASPECT_RATIO_MIN=0.3            # Min aspect ratio
CLUMP_ASPECT_RATIO_MAX=3.0            # Max aspect ratio
CLUMP_DISTANCE_THRESHOLD=50           # Proximity threshold (pixels)

# ===== CLASSIFICATION ACCURACY IMPROVEMENTS =====
# These strategies can improve classification accuracy by 25-35% combined

# Strategy 2: Multi-tier cascade (cost optimization)
# Use progressively expensive models based on confidence
# Tier 1 (Gemini Flash) → Tier 2 (Claude) → Tier 3 (GPT-4)
# Expected: 60-80% cost reduction, 10-15% accuracy improvement
ENABLE_MULTI_TIER=false               # Enable multi-tier classification
TIER1_MODEL=google/gemini-2.5-flash-lite       # Tier 1: Cheap screener
TIER2_MODEL=anthropic/claude-3.5-sonnet        # Tier 2: Accurate classifier
TIER3_MODEL=openai/gpt-4o                      # Tier 3: Premium arbiter (latest)
TIER1_THRESHOLD=0.85                  # Accept Tier 1 if >= 85% confidence
TIER2_THRESHOLD=0.75                  # Accept Tier 2 if >= 75% confidence

# Phase 1: CLIP Disagreement Detection (RECOMMENDED)
# Cross-validates LLM predictions with visual similarity
# Forces escalation when LLM and CLIP strongly disagree
# Expected: 50% reduction in misclassification rate
ENABLE_CLIP_DISAGREEMENT=true         # Enable CLIP disagreement detection (default: true)
CLIP_DISAGREEMENT_THRESHOLD=0.80      # Min CLIP similarity to trust (0.80 = strong match)

# Strategy 3: CLIP visual similarity (visual pre-filtering)
# Use Python CLIP service to suggest visually similar units to LLM
# Expected: 15-20% accuracy improvement
# Requires: Python CLIP service running (see clip_service/README.md)
ENABLE_CLIP=false                     # Enable CLIP visual pre-filtering
CLIP_SERVICE_URL=http://localhost:8000         # CLIP service endpoint
CLIP_TIMEOUT=5000                     # Timeout in ms (5 seconds)

# Strategy 3B: CLIP-Only Classification (EXPERIMENTAL)
# Use ONLY CLIP visual matching (no LLM calls at all)
# Advantages:
#   - SPEED: <100ms per crop (vs 1-5s for LLM)
#   - COST: $0 (vs $0.001-0.10 per LLM call)
#   - ACCURACY: Direct visual matching against reference gallery
# Limitations:
#   - Only works for units in reference gallery (75 images, 15 units)
#   - No natural language reasoning
#   - Requires high-quality reference images
# NOTE: Takes priority over ENABLE_MULTI_TIER if both are true
ENABLE_CLIP_ONLY=false                # Enable CLIP-only mode (bypasses all LLMs)
CLIP_CONFIDENCE_THRESHOLD=0.75        # Min similarity to accept (0.75 = 75% match)
CLIP_UNKNOWN_THRESHOLD=0.70           # Min similarity to avoid "Unknown" (0.70 = 70% match)

# Strategy 4: Few-shot prompting (example-based learning)
# Show labeled example images to LLM before classification
# Expected: 10-15% accuracy improvement
# Requires: Example images in few_shot_examples/ directory
ENABLE_FEW_SHOT=false                 # Enable few-shot example prompting
# Few-shot examples should be placed in: few_shot_examples/faction/unit_name/example.jpg
# Examples are loaded into memory on startup